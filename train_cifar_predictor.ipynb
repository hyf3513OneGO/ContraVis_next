{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T03:42:37.114255Z",
     "start_time": "2025-03-21T03:42:32.426025Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:42:37.128859Z",
     "start_time": "2025-03-21T03:42:37.117850Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.getcwd())",
   "id": "894c81f907604462",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\python\\2024.12.01\\ContraVis_next\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:42:37.942963Z",
     "start_time": "2025-03-21T03:42:37.130859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from models.mlp import MLP\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "origin_predictor = nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)  # CIFAR-10 共有 10 类\n",
    ").to(device)\n",
    "noisy_predictor = nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)  # CIFAR-10 共有 10 类\n",
    ").to(device)\n",
    "optimizer_origin = torch.optim.Adam(origin_predictor.parameters(), lr=1e-3)\n",
    "optimizer_noisy = torch.optim.Adam(noisy_predictor.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ],
   "id": "5621ba5453de2548",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:42:37.956898Z",
     "start_time": "2025-03-21T03:42:37.947493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self,features_np,labels_np):\n",
    "        self.features_np = features_np\n",
    "        self.labels_np = labels_np\n",
    "        assert self.features_np.shape[0] == self.labels_np.shape[0],\"features_np and labels_np must have same shape\"\n",
    "    def __len__(self):\n",
    "        return len(self.labels_np)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features_np[idx],self.labels_np[idx]"
   ],
   "id": "52ee7dfee38f6783",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:42:38.083817Z",
     "start_time": "2025-03-21T03:42:37.958488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pattern_type = \"stripes\"\n",
    "intensity = 0.3\n",
    "raw_features = np.load(r\"E:\\python\\2024.12.01\\ContraVis_next\\dataset\\cifar_resnet_none\\features.npy\")\n",
    "labels = np.load(r\"E:\\python\\2024.12.01\\ContraVis_next\\dataset\\cifar_resnet_none\\labels.npy\")\n",
    "noisy_features = np.load(rf\"E:\\python\\2024.12.01\\ContraVis_next\\dataset\\cifar_resnet_{pattern_type}_{intensity}\\features.npy\")\n",
    "\n",
    "origin_dataset_train = FeatureDataset(raw_features,labels)\n",
    "noisy_dataset_train = FeatureDataset(noisy_features,labels)\n",
    "\n",
    "\n",
    "\n",
    "origin_train_dataloader = torch.utils.data.DataLoader(origin_dataset_train,batch_size=32,shuffle=False)\n",
    "noisy_train_dataloader = torch.utils.data.DataLoader(noisy_dataset_train,batch_size=32,shuffle=False)"
   ],
   "id": "986de49dfec874a0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:42:38.093210Z",
     "start_time": "2025-03-21T03:42:38.086436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_save_dir = \"model_weights/cifar_resnet_classifier\"\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.mkdir(model_save_dir)"
   ],
   "id": "c4d27584c9fca43b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "90e49ebc8436d883"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Origin Predictor",
   "id": "f5852da88bc41b5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:42:38.103018Z",
     "start_time": "2025-03-21T03:42:38.095323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from tqdm import tqdm\n",
    "# \n",
    "# origin_epochs = 12\n",
    "# for epoch in tqdm(range(origin_epochs)):\n",
    "#     origin_predictor.train()\n",
    "#     origin_loss_epoch = 0.0\n",
    "#     for batch in origin_train_dataloader:\n",
    "#         optimizer_origin.zero_grad()\n",
    "#         feature_origin,label_origin = batch\n",
    "#         feature_origin = feature_origin.to(device)\n",
    "#         label_origin = label_origin.to(device)\n",
    "#         pred_origin = origin_predictor(feature_origin)\n",
    "#         loss = criterion(pred_origin,label_origin)\n",
    "#         origin_loss_epoch+=loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer_origin.step()\n",
    "#         \n",
    "#     print(f\"epoch:{epoch} loss:{origin_loss_epoch:.3f}\")\n",
    "#     if epoch % 2 == 0:\n",
    "#         origin_predictor.eval()\n",
    "#         model_weight = origin_predictor.state_dict()\n",
    "#         with torch.no_grad():\n",
    "#             total_origin = 0\n",
    "#             correct_origin = 0\n",
    "#             for batch in origin_train_dataloader:\n",
    "#                 feature_origin,label_origin = batch\n",
    "#                 feature_origin = feature_origin.to(device)\n",
    "#                 label_origin = label_origin.to(device)\n",
    "#                 pred_origin = origin_predictor(feature_origin)\n",
    "#                 _,pred_category = torch.max(pred_origin,dim=1)\n",
    "#                 total_origin += label_origin.size(0)\n",
    "#                 correct_origin += (pred_category == label_origin).sum().item()\n",
    "#                 accuracy_origin = 100*correct_origin / total_origin\n",
    "#             print(f\"epoch:{epoch} Test Accuracy: {accuracy_origin:.2f}%\")\n",
    "#             save_path = os.path.join(model_save_dir,f\"origin_mlp_{epoch}_{loss:.3f}_{accuracy_origin}.pth\")\n",
    "#             torch.save(model_weight,save_path)"
   ],
   "id": "4cb0399df48f0311",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:42:38.116313Z",
     "start_time": "2025-03-21T03:42:38.105087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# origin_predictor.load_state_dict(torch.load(os.path.join(model_save_dir,r\"origin_mlp_10_0.405_85.884.pth\")))\n",
    "# pred_all_origin = []\n",
    "# for batch in origin_train_dataloader:\n",
    "#     feature_origin,label_origin = batch\n",
    "#     feature_origin = feature_origin.to(device)\n",
    "#     label_origin = label_origin.to(device)\n",
    "#     pred_origin = origin_predictor(feature_origin)\n",
    "#     _,pred_category = torch.max(pred_origin,dim=1)\n",
    "#     pred_all_origin.append(pred_category.cpu().numpy())\n",
    "# pred_all_origin_np = np.concatenate(pred_all_origin)\n",
    "# pred_all_origin_save_path = os.path.join(\"dataset/cifar_resnet_none\",\"predictions.npy\")\n",
    "# np.save(pred_all_origin_save_path,pred_all_origin_np)"
   ],
   "id": "1b0a01b55fb2ccc1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train Noisy Predictor",
   "id": "c518c71975d4bc13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:43:48.127409Z",
     "start_time": "2025-03-21T03:42:38.118691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "noisy_epochs = 12\n",
    "for epoch in tqdm(range(noisy_epochs)):\n",
    "    noisy_predictor.train()\n",
    "    noisy_loss_epoch = 0.0\n",
    "    for batch in noisy_train_dataloader:\n",
    "        optimizer_noisy.zero_grad()\n",
    "        feature_noisy,label_noisy = batch\n",
    "        feature_noisy = feature_noisy.to(device)\n",
    "        label_noisy = label_noisy.to(device)\n",
    "        pred_noisy = noisy_predictor(feature_noisy)\n",
    "        loss = criterion(pred_noisy,label_noisy)\n",
    "        noisy_loss_epoch+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_noisy.step()\n",
    "        \n",
    "    print(f\"epoch:{epoch} loss:{noisy_loss_epoch:.3f}\")\n",
    "    if epoch % 2 == 0:\n",
    "        noisy_predictor.eval()\n",
    "        model_weight = noisy_predictor.state_dict()\n",
    "        with torch.no_grad():\n",
    "            total_noisy = 0\n",
    "            correct_noisy = 0\n",
    "            for batch in noisy_train_dataloader:\n",
    "                feature_noisy,label_noisy = batch\n",
    "                feature_noisy = feature_noisy.to(device)\n",
    "                label_noisy = label_noisy.to(device)\n",
    "                pred_noisy = noisy_predictor(feature_noisy)\n",
    "                _,pred_category = torch.max(pred_noisy,dim=1)\n",
    "                total_noisy += label_noisy.size(0)\n",
    "                correct_noisy += (pred_category == label_noisy).sum().item()\n",
    "                accuracy_noisy = 100 * correct_noisy / total_noisy\n",
    "            print(f\"epoch:{epoch} Test Accuracy: {accuracy_noisy:.2f}%\")\n",
    "            save_path = os.path.join(model_save_dir,f\"{pattern_type}_{intensity}_mlp_{epoch}_{loss:.3f}_{accuracy_noisy}.pth\")\n",
    "            torch.save(model_weight,save_path)"
   ],
   "id": "7fcf597abd638e8f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:1201.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▉                                                                            | 1/12 [00:06<01:08,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 Test Accuracy: 76.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▊                                                                     | 2/12 [00:10<00:53,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 loss:991.041\n",
      "epoch:2 loss:921.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▊                                                              | 3/12 [00:17<00:52,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 Test Accuracy: 79.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▋                                                       | 4/12 [00:22<00:43,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 loss:860.115\n",
      "epoch:4 loss:804.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████▌                                                | 5/12 [00:29<00:44,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 Test Accuracy: 82.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 6/12 [00:34<00:34,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 loss:752.155\n",
      "epoch:6 loss:703.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████▍                                  | 7/12 [00:42<00:31,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 Test Accuracy: 83.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████▎                           | 8/12 [00:46<00:23,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 loss:662.040\n",
      "epoch:8 loss:622.253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████▎                    | 9/12 [00:53<00:18,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 Test Accuracy: 83.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████▎             | 10/12 [00:58<00:11,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 loss:581.505\n",
      "epoch:10 loss:544.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 11/12 [01:05<00:06,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 Test Accuracy: 83.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [01:09<00:00,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 loss:513.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:43:50.200351Z",
     "start_time": "2025-03-21T03:43:48.130408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# noisy_predictor.load_state_dict(torch.load(os.path.join(model_save_dir,r\"stripes_0.2_mlp_8_0.224_84.516.pth\")))\n",
    "noisy_predictor.load_state_dict(torch.load(save_path))\n",
    "pred_all_noisy = []\n",
    "for batch in noisy_train_dataloader:\n",
    "    feature_noisy,label_noisy = batch\n",
    "    feature_noisy = feature_noisy.to(device)\n",
    "    label_noisy = label_noisy.to(device)\n",
    "    pred_noisy = noisy_predictor(feature_noisy)\n",
    "    _,pred_category = torch.max(pred_noisy,dim=1)\n",
    "    pred_all_noisy.append(pred_category.cpu().numpy())\n",
    "pred_all_noisy_np = np.concatenate(pred_all_noisy)\n",
    "pred_all_noisy_save_path = os.path.join(f\"dataset/cifar_resnet_{pattern_type}_{intensity}\",\"predictions.npy\")\n",
    "np.save(pred_all_noisy_save_path,pred_all_noisy_np)"
   ],
   "id": "410174b53611ff31",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "gnn_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
